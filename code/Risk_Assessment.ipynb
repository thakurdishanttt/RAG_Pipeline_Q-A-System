{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_community langchain langchain_openai PyPDF2 pypdf chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import PyPDF2\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "from io import BytesIO\n",
    "from uuid import uuid4\n",
    "import chromadb\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm  \n",
    "from time import time\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"pdf_file_path\"\n",
    "CHROMA_PATH = \"chromadb_path\"\n",
    "OPENAI_API_KEY = \"sk-svcacc_bWh4fRY73mDJU-ZFK9T3BlbkF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(file_path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Load and process a local PDF document using PyPDF2.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the local PDF document (e.g., 'path/to/file.pdf').\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of document pages processed from the PDF.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the file path is invalid or the file is not a PDF.\n",
    "        Exception: If there's an error loading or processing the PDF.\n",
    "    \"\"\"\n",
    "    if not file_path.lower().endswith('.pdf'):\n",
    "        raise ValueError(\"Invalid file path. Must be a PDF file.\")\n",
    "\n",
    "    try:\n",
    "        # Read the PDF file from local file system\n",
    "        with open(file_path, 'rb') as file:\n",
    "            pdf_content = BytesIO(file.read())\n",
    "\n",
    "        # Initialize the PDF reader\n",
    "        reader = PyPDF2.PdfReader(pdf_content)\n",
    "        print(f\"PDF loaded from path: {file_path}\")\n",
    "\n",
    "        # Extract text from each page\n",
    "        all_pages = [page.extract_text() for page in reader.pages]\n",
    "\n",
    "        print(f\"Document with {len(all_pages)} pages processed.\")\n",
    "        return all_pages\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading PDF from local file: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocesses the text by removing unwanted symbols and normalizing it.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned and preprocessed text.\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize whitespace (remove extra spaces, newlines, etc.)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def split_text(all_pages: List[str], chunk_size: int = 750, overlap: int = 100) -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits a list of text pages into chunks with overlap, after cleaning and preprocessing the text.\n",
    "\n",
    "    Args:\n",
    "        all_pages (list[str]): A list of text pages.\n",
    "        chunk_size (int, optional): Size of each text chunk. Defaults to 750 characters.\n",
    "        overlap (int, optional): Overlap between chunks. Defaults to 100 characters.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of preprocessed and split text chunks.\n",
    "    \"\"\"\n",
    "    all_chunks = []\n",
    "    for page_text in all_pages:\n",
    "        # Preprocess the text before splitting\n",
    "        clean_text = preprocess_text(page_text)\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        while start < len(clean_text):\n",
    "            end = min(start + chunk_size, len(clean_text))\n",
    "            chunk = clean_text[start:end]\n",
    "            chunks.append(chunk)\n",
    "            start += chunk_size - overlap\n",
    "        all_chunks.extend(chunks)\n",
    "\n",
    "    return all_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_documents_from_chunks(chunks: List[str], source: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Create a list of Document instances from text chunks.\n",
    "\n",
    "    Args:\n",
    "        chunks (List[str]): List of text chunks to be converted into Document instances.\n",
    "        source (str): The source information for the metadata of each Document.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: A list of Document instances.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        doc_id = str(uuid4())  # Generate a unique UUID for each document\n",
    "        document = Document(\n",
    "            page_content=chunk,\n",
    "            metadata={\"source\": source},\n",
    "            id=doc_id,\n",
    "        )\n",
    "        documents.append(document)\n",
    "\n",
    "    return documents\n",
    "\n",
    "def add_documents_to_vector_store(chunks: List[str], source: str):\n",
    "    \"\"\"\n",
    "    Create documents from text chunks and add them to the vector store.\n",
    "\n",
    "    Args:\n",
    "        chunks (List[str]): List of text chunks to be converted into Document instances.\n",
    "        source (str): The source information for the metadata of each Document.\n",
    "    \"\"\"\n",
    "    embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n",
    "\n",
    "    # Create documents from chunks\n",
    "    documents = create_documents_from_chunks(chunks, source)\n",
    "    CHROMA_PATH = \"/content/chroma_vectordb_2\" \n",
    "\n",
    "    # Generate unique IDs for each document\n",
    "    uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "\n",
    "    # Create and persist the vector store\n",
    "    db = Chroma.from_documents(documents, embeddings, persist_directory=CHROMA_PATH, ids=uuids)\n",
    "\n",
    "    # Reload the Chroma database\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embeddings)\n",
    "    print(f\"Added {len(documents)} documents to the vector store.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_and_invoke(query_text: str):\n",
    "    \"\"\"\n",
    "    Retrieve top chunks from the vector store and invoke the OpenAI API.\n",
    "\n",
    "    Args:\n",
    "        query_text (str): The question to be answered.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated response from the OpenAI API.\n",
    "        str: The context used for generating the response.\n",
    "    \"\"\"\n",
    "    embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n",
    "    vectordb = Chroma(persist_directory=\"/content/chroma_vectordb_2\", embedding_function=embeddings)\n",
    "    retriever = vectordb.as_retriever(search_kwargs={\"k\": 7})\n",
    "    results = retriever.invoke(query_text)\n",
    "\n",
    "    # Construct the context from the top unique documents\n",
    "    unique_texts = set(doc.page_content for doc in results)\n",
    "    context_text = \"\\n\\n---\\n\\n\".join(unique_texts)\n",
    "\n",
    "    # Define the prompt template\n",
    "    PROMPT_TEMPLATE = \"\"\"\n",
    "    # CONTEXT\n",
    "    You are given a question from an audit inspection and the relevant content related to that question. Your task is to analyze the content to summarize the findings, provide a clear conclusion, and deliver a one-word result based on the audit's compliance with the question.\n",
    "    Question: {question}\n",
    "    Content: {content}\n",
    "    \n",
    "    # OBJECTIVE\n",
    "    Analyze the provided context (question and related content) and return three outputs:\n",
    "    \n",
    "    Findings: Summarize the relevant sections and subsections from the content.\n",
    "    Conclusion: Provide a full-sentence conclusion based on the findings that directly answers the question.\n",
    "    Result: Give a one-word result based on the conclusion: Pass, Fail, NA (Not Applicable), or Uncertain.\n",
    "    \n",
    "    # STYLE\n",
    "    Findings: Summarize, rephrase, and clarify the key points from the content.\n",
    "    Conclusion: Provide a clear, concise sentence that directly answers the question.\n",
    "    Result: Deliver the result in one word (Pass, Fail, NA, or Uncertain).\n",
    "    \n",
    "    # TONE\n",
    "    Formal, concise, and objective. Provide clear and accurate responses suitable for audit reviewers.\n",
    "    \n",
    "    # AUDIENCE\n",
    "    The target audience is audit file reviewers who will use your analysis to determine the company's compliance with the audit question.\n",
    "    \n",
    "    # RESPONSE FORMAT\n",
    "    Return your analysis in JSON format with the following structure:\n",
    "    Findings: A summary of relevant sections and subsections.\n",
    "    Conclusion: A clear sentence summarizing the answer to the question.\n",
    "    Result: A one-word result (Pass, Fail, NA, or Uncertain).\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PROMPT_TEMPLATE.format(question=query_text, content=context_text)\n",
    "\n",
    "    # Generate a response using the OpenAI API\n",
    "    client = OpenAI(api_key=\"YOUR_API_KEY\")\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0,\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "    )\n",
    "\n",
    "    response_text = completion.choices[0].message.content\n",
    "    return response_text, context_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_content = load_documents(FILE_PATH)\n",
    "chunks = split_text(pdf_content)\n",
    "vectordb = add_documents_to_vector_store(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import json  # Import the json module\n",
    "\n",
    "def process_questions_and_save_results(csv_path: str, output_csv_path: str):\n",
    "    \"\"\"\n",
    "    Load questions from a CSV file, process them, and save the generated answers to a new CSV file.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path to the CSV file containing questions.\n",
    "        output_csv_path (str): Path to the output CSV file for saving results.\n",
    "    \"\"\"\n",
    "    # Load questions from CSV\n",
    "    questions_df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Initialize result list and start timer\n",
    "    results = []\n",
    "    start_time = time()\n",
    "\n",
    "    # Iterate through questions and get answers with tqdm progress bar\n",
    "    for index, row in tqdm(questions_df.iterrows(), total=questions_df.shape[0], desc=\"Processing Questions\"):\n",
    "        question = row['Question']\n",
    "        try:\n",
    "            # Assuming retrieve_and_invoke() returns a JSON string\n",
    "            json_output_str = retrieve_and_invoke(question)  # Call the function to get the JSON output\n",
    "            \n",
    "            # Parse the JSON string into a Python dictionary\n",
    "            json_output = json.loads(json_output_str)\n",
    "\n",
    "            # Extract relevant parts from the JSON output\n",
    "            findings = json_output.get('Findings', '')\n",
    "            conclusion = json_output.get('Conclusion', '')\n",
    "            result = json_output.get('Result', '')\n",
    "\n",
    "            # Append structured data to results\n",
    "            results.append({\n",
    "                'Question': question,\n",
    "                'Findings': findings,\n",
    "                'Conclusion': conclusion,\n",
    "                'Result': result\n",
    "            })\n",
    "        except (ValueError, json.JSONDecodeError):\n",
    "            results.append({\n",
    "                'Question': question, \n",
    "                'Findings': \"Error occurred during processing.\",\n",
    "                'Conclusion': \"Provided context does not contain the answer.\",\n",
    "                'Result': \"Uncertain\"\n",
    "            })\n",
    "\n",
    "    # End timer\n",
    "    end_time = time()\n",
    "    print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    # Create DataFrame from results and save to CSV\n",
    "    result_df = pd.DataFrame(results)\n",
    "    result_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "    # Print the result DataFrame\n",
    "    print(result_df)\n",
    "\n",
    "# Example usage\n",
    "process_questions_and_save_results(\"/content/questions_conclusions_results.csv\", \"answers_on_new_qset__Henan_SMETA.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "74d1634a3fba8ee8e03ea727bc58cab47a79b664c589548ecd06b52edaf2ee7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
