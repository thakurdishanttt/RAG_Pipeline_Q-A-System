{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E__LOyfSnVSK"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_community langchain langchain_openai PyPDF2 pypdf chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "0010NjKKnYM7",
        "outputId": "d611329d-5750-4b99-8ce0-92cde3e95d43"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'PyPDF2'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a6ec01412178>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mPyPDF2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'PyPDF2'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import PyPDF2\n",
        "from typing import List\n",
        "from pathlib import Path\n",
        "from io import BytesIO\n",
        "\n",
        "def load_documents(file_path: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Loads and processes a local PDF document using PyPDF2.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the local PDF document (e.g., 'path/to/file.pdf').\n",
        "\n",
        "    Returns:\n",
        "        list[str]: A list of document pages processed from the PDF.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the file path is invalid or the file is not a PDF.\n",
        "        Exception: If there's an error loading or processing the PDF.\n",
        "    \"\"\"\n",
        "    if not file_path.lower().endswith('.pdf'):\n",
        "        raise ValueError(\"Invalid file path. Must be a PDF file.\")\n",
        "\n",
        "    try:\n",
        "        # Read the PDF file from local file system\n",
        "        with open(file_path, 'rb') as file:\n",
        "            pdf_content = BytesIO(file.read())\n",
        "\n",
        "        # Initialize the PDF reader\n",
        "        reader = PyPDF2.PdfReader(pdf_content)\n",
        "        print(f\"PDF loaded from path: {file_path}\")\n",
        "\n",
        "        # Extract text from each page\n",
        "        all_pages = [page.extract_text() for page in reader.pages]\n",
        "\n",
        "        print(f\"Document with {len(all_pages)} pages processed.\")\n",
        "        return all_pages\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading PDF from local file: {e}\")\n",
        "        raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ob3YRVuna0L"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import List\n",
        "\n",
        "def preprocess_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Preprocesses the text by removing unwanted symbols and normalizing it.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to be cleaned.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned and preprocessed text.\n",
        "    \"\"\"\n",
        "\n",
        "    # Normalize whitespace (remove extra spaces, newlines, etc.)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def split_text(all_pages: List[str], chunk_size: int = 750, overlap: int = 100) -> List[str]:\n",
        "    \"\"\"\n",
        "    Splits a list of text pages into chunks with overlap, after cleaning and preprocessing the text.\n",
        "\n",
        "    Args:\n",
        "        all_pages (list[str]): A list of text pages.\n",
        "        chunk_size (int, optional): Size of each text chunk. Defaults to 750 characters.\n",
        "        overlap (int, optional): Overlap between chunks. Defaults to 100 characters.\n",
        "\n",
        "    Returns:\n",
        "        list[str]: A list of preprocessed and split text chunks.\n",
        "    \"\"\"\n",
        "    all_chunks = []\n",
        "    for page_text in all_pages:\n",
        "        # Preprocess the text before splitting\n",
        "        clean_text = preprocess_text(page_text)\n",
        "        chunks = []\n",
        "        start = 0\n",
        "        while start < len(clean_text):\n",
        "            end = min(start + chunk_size, len(clean_text))\n",
        "            chunk = clean_text[start:end]\n",
        "            chunks.append(chunk)\n",
        "            start += chunk_size - overlap\n",
        "        all_chunks.extend(chunks)\n",
        "\n",
        "    return all_chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsjKdaM-nc-q"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from uuid import uuid4\n",
        "from typing import List\n",
        "from langchain_core.documents import Document\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "def create_documents_from_chunks(chunks: List[str], source: str) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Create a list of Document instances from text chunks.\n",
        "\n",
        "    Args:\n",
        "        chunks (List[str]): List of text chunks to be converted into Document instances.\n",
        "        source (str): The source information for the metadata of each Document.\n",
        "\n",
        "    Returns:\n",
        "        List[Document]: A list of Document instances.\n",
        "    \"\"\"\n",
        "    documents = []\n",
        "\n",
        "    for chunk in chunks:\n",
        "        doc_id = str(uuid4())  # Generate a unique UUID for each document\n",
        "        document = Document(\n",
        "            page_content=chunk,\n",
        "            metadata={\"source\": source},\n",
        "            id=doc_id,\n",
        "        )\n",
        "        documents.append(document)\n",
        "\n",
        "    return documents\n",
        "\n",
        "def add_documents_to_vector_store(chunks: List[str], source: str):\n",
        "    \"\"\"\n",
        "    Create documents from text chunks and add them to the vector store.\n",
        "\n",
        "    Args:\n",
        "        chunks (List[str]): List of text chunks to be converted into Document instances.\n",
        "        source (str): The source information for the metadata of each Document.\n",
        "        vector_store: The vector store instance to which documents will be added.\n",
        "    \"\"\"\n",
        "    embeddings = OpenAIEmbeddings(api_key=\"sk-Wh4fRY73mDJU-ZFK9T3BlbkFJbZQ8stps9eneBF-7Utao1uI_qXJjcBBkxqFhPyjrYOktx88twVQazSVf7PwA\")\n",
        "\n",
        "\n",
        "    # Create documents from chunks\n",
        "    documents = create_documents_from_chunks(chunks, source)\n",
        "    CHROMA_PATH = \"/content/chroma_vectordb_Anhui\"\n",
        "    # Generate unique IDs for each document\n",
        "    uuids = [str(uuid4()) for _ in range(len(documents))]\n",
        "\n",
        "    db = Chroma.from_documents(documents, embeddings, persist_directory=CHROMA_PATH,ids = uuids)\n",
        "\n",
        "\n",
        "    # Reload the Chroma database\n",
        "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embeddings)\n",
        "    print(f\"Added {len(documents)} documents to the vector store.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcO4tjp1njjk"
      },
      "outputs": [],
      "source": [
        "documents = load_documents(\"/content/Anhui Forestwind_Report-SMETA-ET2112060708-Dec.6-7,2021 (2) (1).pdf\")\n",
        "chunks = split_text(documents)\n",
        "add_documents_to_vector_store(chunks, \"/content/Anhui Forestwind_Report-SMETA-ET2112060708-Dec.6-7,2021 (2) (1).pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNZrAud6nl98"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json  # Ensure you have this import at the top\n",
        "from openai import OpenAI\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "def retrieve_and_invoke(query_text: str):\n",
        "    \"\"\"\n",
        "    Retrieve top chunks from the vector store with their scores and invoke the OpenAI API.\n",
        "\n",
        "    Args:\n",
        "        query_text (str): The question to be answered.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the findings, conclusion, result, and confidence score.\n",
        "        str: The context used for generating the response.\n",
        "    \"\"\"\n",
        "    embeddings = OpenAIEmbeddings(api_key=\"sk-svcacct-RBat5Y-ZFK9T3BlbkFJbZQ8stps9eneBF-7Utao1uI_qXJjcBBkxqFhPyjrYOktx88twVQazSVf7PwA\")\n",
        "\n",
        "    # Load the Chroma vector store from the persistent directory\n",
        "    vectordb = Chroma(persist_directory=\"/content/chroma_vectordb_Anhui\", embedding_function=embeddings)\n",
        "\n",
        "    # Use similarity search with relevance scores (no need to add documents)\n",
        "    results = vectordb.similarity_search_with_relevance_scores(query_text, k=7)\n",
        "\n",
        "    # Collect chunks and scores\n",
        "    scored_chunks = [(doc.page_content, score) for doc, score in results]\n",
        "\n",
        "    # Construct the context from the top unique documents\n",
        "    unique_texts = set(chunk for chunk, _ in scored_chunks)\n",
        "    context_text = \"\\n\\n---\\n\\n\".join(unique_texts)\n",
        "\n",
        "    # Define the prompt template\n",
        "    PROMPT_TEMPLATE = \"\"\"\n",
        "    # CONTEXT\n",
        "    You are given a question from an audit inspection and the relevant content related to that question. Your task is to analyze the content to summarize the findings, provide a clear conclusion, and deliver a one-word result based on the audit's compliance with the question.\n",
        "    Question: {question}\n",
        "    Content: {content}\n",
        "\n",
        "    # OBJECTIVE\n",
        "    Analyze the provided context (question and related content) and return four outputs:\n",
        "\n",
        "    Findings: Summarize the relevant sections and subsections from the content.\n",
        "    Conclusion: Provide a full-sentence conclusion based on the findings that directly answers the question.\n",
        "    Result: Give a one-word result based on the conclusion: Pass, Fail, NA (Not Applicable), or Uncertain.\n",
        "    Confidence_Perc: Provide a confidence score (out of 100) that reflects the certainty of the conclusion.\n",
        "\n",
        "    # STYLE\n",
        "    Findings: Summarize, rephrase, and clarify the key points from the content.\n",
        "    Conclusion: Provide a clear, concise sentence that directly answers the question.\n",
        "    Result: Deliver the result in one word (Pass, Fail, NA, or Uncertain).\n",
        "    Confidence_Perc: A number between 0 and 100 representing the model's confidence in the answer.\n",
        "\n",
        "    # TONE\n",
        "    Formal, concise, and objective. Provide clear and accurate responses suitable for audit reviewers.\n",
        "\n",
        "    # AUDIENCE\n",
        "    The target audience is audit file reviewers who will use your analysis to determine the company's compliance with the audit question.\n",
        "\n",
        "    # RESPONSE FORMAT\n",
        "    Return your analysis in JSON format with the following structure, directly in json format so that I can load into a json file (Dont add unnecessary extra escape characters (\\n for newlines and \\'  which will make it hard to load to json)\n",
        "    Check the syntax of json properly and give the below:\n",
        "    Findings: A summary of relevant sections and subsections.\n",
        "    Conclusion: A clear sentence summarizing the answer to the question.\n",
        "    Result: A one-word result (Pass, Fail, NA, or Uncertain).\n",
        "    Confidence_Perc: A number out of 100 indicating how confident the model is in the conclusion.\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = PROMPT_TEMPLATE.format(question=query_text, content=context_text)\n",
        "\n",
        "    # Generate a response using the OpenAI API\n",
        "    client = OpenAI(api_key=\"sk-svcacct-RBat5Y6n4uTDNL03C7y-s8OK0bPZFK9T3BlbkFJbZQ8stps9eneBF-7Utao1uI_qXJjcBBkxqFhPyjrYOktx88twVQazSVf7PwA\")\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        temperature=0,\n",
        "        messages=[{\"role\": \"system\", \"content\": prompt}],\n",
        "    )\n",
        "\n",
        "    response_text = completion.choices[0].message.content\n",
        "    # Parse the JSON response\n",
        "    try:\n",
        "        response_json = json.loads(response_text)\n",
        "        findings = response_json.get(\"Findings\", \"\")\n",
        "        conclusion = response_json.get(\"Conclusion\", \"\")\n",
        "        result = response_json.get(\"Result\", \"\")\n",
        "        confidence_perc = response_json.get(\"Confidence_Perc\", 0)  # Extract confidence score\n",
        "\n",
        "        # Ensure findings and conclusion are strings and strip them\n",
        "        findings = str(findings).strip()\n",
        "        conclusion = str(conclusion).strip()\n",
        "\n",
        "        # Ensure result is a string and normalize 'N/A' to 'NA'\n",
        "        result = str(result).strip()\n",
        "\n",
        "        # Normalize 'N/A' to 'NA'\n",
        "        if result == \"N/A\":\n",
        "            result = \"NA\"\n",
        "\n",
        "        # Validate the 'result' field\n",
        "        valid_results = {\"Pass\", \"Fail\", \"NA\", \"Uncertain\"}\n",
        "        if result not in valid_results:\n",
        "            raise ValueError(f\"Invalid result value: {result}. Must be one of {valid_results}.\")\n",
        "\n",
        "        return {\n",
        "            \"findings\": findings,\n",
        "            \"conclusion\": conclusion,\n",
        "            \"result\": result,\n",
        "            \"confidence_perc\": confidence_perc  # Include confidence score in the output\n",
        "        }, context_text\n",
        "\n",
        "    except (ValueError, json.JSONDecodeError):\n",
        "        return {\n",
        "            'findings': \"Error occurred during processing.\",\n",
        "            'conclusion': \"Provided context does not contain the answer.\",\n",
        "            'result': \"Uncertain\",\n",
        "            'confidence_perc': 0  # Default confidence score on error\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'findings': \"Error occurred during processing.\",\n",
        "            'conclusion': \"Provided context does not contain the answer.\",\n",
        "            'result': \"Uncertain\",\n",
        "            'confidence_perc': 0  # Default confidence score on error\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from time import time\n",
        "import json  # Import the json module\n",
        "\n",
        "def process_questions_and_save_results(csv_path: str, output_csv_path: str):\n",
        "    \"\"\"\n",
        "    Load questions from a CSV file, process them, and save the generated answers to a new CSV file.\n",
        "\n",
        "    Args:\n",
        "        csv_path (str): Path to the CSV file containing questions.\n",
        "        output_csv_path (str): Path to the output CSV file for saving results.\n",
        "    \"\"\"\n",
        "    # Load questions from CSV\n",
        "    questions_df = pd.read_csv(csv_path)\n",
        "    \n",
        "    # Initialize result list and start timer\n",
        "    results = []\n",
        "    start_time = time()\n",
        "\n",
        "    # Iterate through questions and get answers with tqdm progress bar\n",
        "    for index, row in tqdm(questions_df.iterrows(), total=questions_df.shape[0], desc=\"Processing Questions\"):\n",
        "        question = row['Question']\n",
        "        try:\n",
        "            # Assuming retrieve_and_invoke() returns a JSON string\n",
        "            json_output_str = retrieve_and_invoke(question)  # Call the function to get the JSON output\n",
        "            \n",
        "            # Parse the JSON string into a Python dictionary\n",
        "            json_output = json.loads(json_output_str)\n",
        "\n",
        "            # Extract relevant parts from the JSON output\n",
        "            findings = json_output.get('Findings', '')\n",
        "            conclusion = json_output.get('Conclusion', '')\n",
        "            result = json_output.get('Result', '')\n",
        "            confidence_perc = json_output.get('Confidence_Perc', '')\n",
        "\n",
        "            # Append structured data to results\n",
        "            results.append({\n",
        "                'Question': question,\n",
        "                'Findings': findings,\n",
        "                'Conclusion': conclusion,\n",
        "                'Result': result,\n",
        "                'Confidence_Perc' : confidence_perc\n",
        "                \n",
        "            })\n",
        "        except (ValueError, json.JSONDecodeError):\n",
        "            results.append({\n",
        "                'Question': question, \n",
        "                'Findings': \"Error occurred during processing.\",\n",
        "                'Conclusion': \"Provided context does not contain the answer.\",\n",
        "                'Result': \"Uncertain\",\n",
        "                'Confidence_Perc': \"0\"\n",
        "            })\n",
        "\n",
        "    # End timer\n",
        "    end_time = time()\n",
        "    print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "    # Create DataFrame from results and save to CSV\n",
        "    result_df = pd.DataFrame(results)\n",
        "    result_df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "    # Print the result DataFrame\n",
        "    print(result_df)\n",
        "\n",
        "# Example usage\n",
        "process_questions_and_save_results(\"/content/questions_conclusions_results.csv\", \"answers_on_new_qset__Henan_SMETA.csv\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
